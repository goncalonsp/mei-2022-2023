---
title: "Max Flow problem"
output: 
  html_notebook: 
    toc: yes
    toc_float:
      collapsed: false
    number_sections: true
---

# Dependencies

```{r Libraries}
#library(readr)
#install.packages("ggplot2")
#library(ggplot2)
#install.packages("scatterplot3d", dependencies = TRUE)
library(scatterplot3d)
#install.packages("rgl")
library(rgl)
options(rgl.printRglwidget = TRUE)
```

```{r Compute colormap function}
compute_colormap <- function(d) {
  zlim <- range(d)
  zlen <- zlim[2] - zlim[1] + 1
  colorlut <- terrain.colors(zlen)
  colorlut[d - zlim[1] + 1]
}
```

# Max Flow problem data set

All data sets have the following variables:

-   $n$ = number of vertices
-   $p$ = arc probability (0 \<= p \<= 1)
-   $r$ = maximum range of capacity
-   $s$ = random seed
-   $Dinic$ = execution time of the Dinic.cpp algorithm in secs
-   $EK$ = execution time of the EK.cpp algorithm in secs
-   $MPM$ = execution time of the MPM.cpp algorithm in secs

For the number of arcs $|A|$, we will use $|A|=np$.

# Dinic

```{r Load Dinic data set}
df <- read.csv(file = "results_n1000-10000_p0.3-0.7_6seed_dinic.csv", header=TRUE)
df$a <- df$n * df$p
```

## Analysis

```{r Summary of Dinic data}
summary(df)
```

```{r}
boxplot(df$Dinic~df$p, main="Dinic.cpp")
```

As can be seen in the 3D graph below, as *n* increases the execution time increases. We can also observe that, for larger values of *n*, as *p* increases the variance of the execution time decreases.

```{r Visualise Dinic n and p}
plot3d(x = df$n, y = df$p, z = df$Dinic,
       xlab = 'n', ylab = 'p', zlab = 'Dinic',
       col=c("blue"))
```

We opted to always use the original variables $n$ and $p$ instead of using $a$. The reason is that since $a$ depends on both $n$ and $p$ graphical visualizations would appear a bit off as can be seen by the following graph:

```{r Visualise Dinic n and a}
plot3d(x = df$n, y = df$a, z = df$Dinic,
       xlab = 'n', ylab = 'a', zlab = 'Dinic',
       col=c("blue"))
```

## Linear Regression

We know that Dinic has a time complexity of $O(|A||V|^2)$, so we start by fitting it to a quadratic function of the same form:

```{r Dinic regression using a * n^2}
lr <- lm(Dinic ~ I(n*p*n^2), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$Dinic,
       xlab = 'n', ylab = 'p', zlab = 'Dinic',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```
Let's try making $|A|$ a separate coeficient:

```{r Dinic regression using a + n^2}
lr <- lm(Dinic ~ I(n*p) + I(n^2), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$Dinic,
       xlab = 'n', ylab = 'p', zlab = 'Dinic',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

It get's us a better result. For completeness we also try using a linear model. We can see that it performs worse as expected.

```{r Dinic regression using a + n}
lr <- lm(Dinic ~ I(n*p) + n, data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$Dinic,
       xlab = 'n', ylab = 'p', zlab = 'Dinic',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

# MPM

```{r Load MPM data set}
df <- read.csv(file = "results_n1000-5000_p0.3-0.7_6seed_mpm.csv", header=TRUE)
df$a <- df$n * df$p
```

## Analysis

```{r Summary of MPM data}
summary(df)
```

```{r}
boxplot(df$MPM~df$p, main="MPM.cpp")
```

Similar to Dinic, as *n* increases the execution time increases. On the other hand we see that, more than impacting the variance, *p* has a considerable effect on the execution time. As *p* increases, the execution time also grows. This leads us to question the assumption that MPM would follow a time complexity of $O(|V|^3)$.

```{r Visualise MPM in 3D}
plot3d(x = df$n, y = df$p, z = df$MPM,
       xlab = 'n', ylab = 'p', zlab = 'MPM',
       col=c("blue"))
```

## Linear Regression

Even though we have already stated that MPM won't follow the expected $O(|V|^3)$ time complexity, lets start by trying it anyway:

```{r MPM regression using n^3}
lr <- lm(MPM ~ I(n^3), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$MPM,
       xlab = 'n', ylab = 'p', zlab = 'MPM',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

As we saw with the visualization, $p$ has impact on the time took and so we must consider it in the model.
Let's add a dependency on $a$:

```{r MPM regression using a + n^3}
lr <- lm(MPM ~ I(n*p) + I(n^3), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$MPM,
       xlab = 'n', ylab = 'p', zlab = 'MPM',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

Gets us a better result but there's still room for improvement. Let's also make $|A|$ cubic:

```{r MPM regression using a^3 + n^3}
lr <- lm(MPM ~ I((n*p)^3) + I(n^3), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$MPM,
       xlab = 'n', ylab = 'p', zlab = 'MPM',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

Clearly better than the previous results.

# EK

```{r Load EK data set}
df <- read.csv(file = "results_n400-1500_p0.1-0.9_6seed_ek.csv", header=TRUE)
df$a <- df$n * df$p
```

## Analysis

```{r Summary of EK data}
summary(df)
```

```{r}
boxplot(df$EK~df$p, main="EK.cpp")
```

The EK algorithm seems to behave similar to MPM. Both $n$ and $p$ impact the execution time as they increase.

```{r Visualise EK in 3D}
plot3d(x = df$n, y = df$p, z = df$EK,
       xlab = 'n', ylab = 'p', zlab = 'EK',
       col=c("blue"))
```

## Linear Regression

We expect that EK follows a time complexity of $O(|A|^2|V|)$, so we start by fitting it:

```{r EK regression using a^2 * n}
lr <- lm(EK ~ I((n * p)^2 * n), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$EK,
       xlab = 'n', ylab = 'p', zlab = 'EK',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

Let's try to instead have $n$ as an new term:

```{r EK regression using a^2 + n}
lr <- lm(EK ~ I((n * p)^2) + n, data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$EK,
       xlab = 'n', ylab = 'p', zlab = 'EK',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

Get's worse. Let's try also removing the $n$ part, thus using only $|A|^2$:

```{r EK regression using a^2}
lr <- lm(EK ~ I((n * p)^2), data = df)
summary(lr)
plot(lr)
n_gen <- seq(min(df$n), max(df$n), length.out = 100)
p_gen <- seq(min(df$p), max(df$p), length.out = 100)
pred <- outer(X=n_gen, Y=p_gen, FUN=function(x,y)predict(lr, data.frame(n = x, p = y), type = "response"))
plot3d(x = df$n, y = df$p, z = df$EK,
       xlab = 'n', ylab = 'p', zlab = 'EK',
       col=c("blue"))
col <- compute_colormap(pred)  
surface3d(x = n_gen, y = p_gen, z = pred, color = col, back = "lines")
```

Better but still worse than the first model.

# Notes

- Dinic ~ I(n * p * n^2) gives a very low Adjusted $R^2$ value of 0.3506
